{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PATH_TO_DATA = \"../../data/\"\n",
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.word2id = {}\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        #self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            j = 0\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word not in [',','.','#','?','!',',','\"',\"'\",':',';','(',')', '/']: # Remove punctuation\n",
    "                    self.word2vec[word] = np.fromstring(vec, sep=' ') \n",
    "                    self.word2id[word] = j\n",
    "                    j += 1\n",
    "                # Code to not differentiate word and word.lower()\n",
    "                #self.word2vec[word.lower()] = np.fromstring(vec, sep=' ') if not (word.lower() in self.word2vec.keys()) else (np.fromstring(vec, sep=' ')+self.word2vec[word.lower()])/2\n",
    "                #self.word2id[word.lower()] = i if not (word.lower() in self.word2id.keys()) else self.word2id[word.lower()]\n",
    "                if j == (nmax):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        if w in self.word2vec.keys():\n",
    "            # Create a list with the score of each word\n",
    "            most_similar_list = [self.score(word,w) for word in self.word2vec.keys()]\n",
    "            # return the 5 most similar words\n",
    "            most_similar_list_argsort = np.argsort(most_similar_list)\n",
    "            return [self.id2word[most_similar_list_argsort[-k-2]] for k in range(K)]\n",
    "        else:\n",
    "            return w+' is missing of vocabulary'\n",
    "       \n",
    "    \n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        try:\n",
    "            return np.dot(self.word2vec[w1], self.word2vec[w2])/((np.linalg.norm(self.word2vec[w1]))*(np.linalg.norm(self.word2vec[w2])))\n",
    "        except KeyError as e:\n",
    "            return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 'paris'\n",
      "germany berlin 'germany'\n",
      "['cats', 'kitty', 'kitten', 'Cat', 'dog']\n",
      "['dogs', 'puppy', 'Dog', 'canine', 'pup']\n",
      "['dog', 'Dogs', 'puppies', 'cats', 'canine']\n",
      "paris is missing of vocabulary\n",
      "germany is missing of vocabulary\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=25000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "\n",
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # compute the mean of word2vec values for each words in a sent\n",
    "                #sentemb.append(np.mean([self.w2v.word2vec[word] for word in sent.split(' ') if word in self.w2v.word2vec.keys()], axis=0))\n",
    "                vectors = np.array([self.w2v.word2vec[word] for word in sent.split(' ') if word in self.w2v.word2vec.keys()])\n",
    "                if vectors.any():\n",
    "                    sentemb.append(np.mean(vectors, axis=0))\n",
    "                else:\n",
    "                    sentemb.append(np.zeros(300))\n",
    "                \n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                \n",
    "                # Create a vector with words of each sentences, and their weights from the idf dict\n",
    "                vectors = [self.w2v.word2vec[word] for word in sent.split(' ') if word in self.w2v.word2vec.keys()]\n",
    "                weights = [idf[word] for word in sent.split(' ') if word in self.w2v.word2vec.keys()]\n",
    "\n",
    "                if np.sum(weights)>0:\n",
    "                    sentemb.append(np.average(vectors, axis=0, weights=weights))\n",
    "                else:\n",
    "                    sentemb.append(np.zeros(300))\n",
    "                \n",
    "        return sentemb\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5, verbose=False):\n",
    "        # get most similar sentences and **print** them\n",
    "        # calculate the average vector for all words in every sentence/document \n",
    "        # and use cosine similarity between vectors\n",
    "        #keys = self.encode(sentences, idf)\n",
    "        #query = self.encode([s], idf)\n",
    "        \n",
    "        # Create many batches to limit memory usage\n",
    "        size_total = len(sentences)\n",
    "        batch_size = 3000\n",
    "        batch_step = int(size_total/batch_size)\n",
    "        batch_last = size_total - batch_size * batch_step\n",
    "        \n",
    "        most_similar_list = [] # Create a list of tuple (sentence, score)\n",
    "        \n",
    "        # Some verbose\n",
    "        time1=time.time()\n",
    "        timer1 = True\n",
    "        timer2 = True\n",
    "        timer3 = True\n",
    "        # Loop on batches       \n",
    "        for batch in range(batch_step):\n",
    "            # Add some timers\n",
    "            if verbose==True:\n",
    "                if (batch/batch_step > 1/4)&(timer1):\n",
    "                    time2=time.time()\n",
    "                    print('25% done in ', int(time2-time1), ' sec!')\n",
    "                    timer1=False\n",
    "                elif (batch/batch_step > 1/2)&(timer2):\n",
    "                    time3=time.time()\n",
    "                    print('50% done in ', int(time3-time2), ' sec!')\n",
    "                    timer2=False\n",
    "                elif (batch/batch_step > 3/4)&(timer3):\n",
    "                    time4=time.time()\n",
    "                    print('75% done in ', int(time4-time3), ' sec!')\n",
    "                    timer3=False\n",
    "            \n",
    "            # Create a list with scores of all words in the batch\n",
    "            most_similar_list_per_batch = []\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning) # Ignore a RuntimeWarning\n",
    "                for sent in sentences[batch*batch_size:(batch+1)*batch_size]:\n",
    "                    _score = self.score(sent, s, idf) # Compute the score between 2 sentences with self.score\n",
    "                    most_similar_list_per_batch.append(_score if np.any(_score>0) else 0) # Append the score to the list or append 0\n",
    "                \n",
    "                \n",
    "            most_similar_list_per_batch_argsort = np.argsort(most_similar_list_per_batch) # Create argsort list\n",
    "            \n",
    "            # Add the 6 similar sentences of this batch to a general list (6 to avoid an if condition to remove the actual sentence)\n",
    "            for k in range(K+1):\n",
    "                most_similar_list.append((sentences[batch*batch_size+most_similar_list_per_batch_argsort[-k-1]], most_similar_list_per_batch[most_similar_list_per_batch_argsort[-k-1]]))\n",
    "            \n",
    "        # Last batch (same as before)\n",
    "        if len(sentences[batch_size * batch_step:])>6: # If the last batch is longer than 6\n",
    "            most_similar_list_per_batch = []\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                for sent in sentences[batch_size * batch_step:]:\n",
    "                    _score = self.score(sent, s, idf)\n",
    "                    most_similar_list_per_batch.append(_score if np.any(_score>0) else 0)\n",
    "\n",
    "            most_similar_list_per_batch_argsort = np.argsort(most_similar_list_per_batch)\n",
    "        \n",
    "            for k in range(K+1):\n",
    "                most_similar_list.append((sentences[batch_size*batch_step+most_similar_list_per_batch_argsort[-k-1]], most_similar_list_per_batch[most_similar_list_per_batch_argsort[-k-1]]))\n",
    "        else: # Add all the last batch if his length is smaller than 6\n",
    "            for sent in sentences[batch_size * batch_step:]:\n",
    "                most_similar_list.append((sent, self.score(sent, s, idf) if np.any(self.score(sent, s, idf))>0 else 0))\n",
    "                \n",
    "                \n",
    "                \n",
    "        # Sort the most_similar_list (sentence, word) to keep the 5 highest scores\n",
    "        most_similar_list.sort(key=lambda tup: tup[1])\n",
    "        \n",
    "        # Some verbose\n",
    "        time4=time.time()\n",
    "        if verbose==True:\n",
    "            print(\"100% done in \", int(time4-time3), ' sec.')\n",
    "            print('Total time : ', int(time4-time1), ' sec.')\n",
    "            print('-'*30)\n",
    "            print('The 5 most similar sentences are :')\n",
    "        \n",
    "        # Get and Print the K similar sentences\n",
    "        K_similar_sentences = []\n",
    "        for k in range(K):\n",
    "            K_similar_sentences.append(most_similar_list[-k-2][0])\n",
    "            print(most_similar_list[-k-2][0])\n",
    "        \n",
    "        return K_similar_sentences\n",
    "    \n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        if s1 and s2:\n",
    "            return np.dot(self.encode([s1, s2], idf)[0], self.encode([s1, s2], idf)[1]) / (\n",
    "                (np.linalg.norm(self.encode([s1, s2], idf)[0] * np.linalg.norm(self.encode([s1, s2], idf)[1]))))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "        \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        # Let’s compute the Idf for every word in the vocabulary\n",
    "        word_idf = np.zeros(len(self.w2v.word2id))\n",
    "        sentences_only_with_voc = [[word for word in sent.split(' ') if word in self.w2v.word2id.keys()] for sent in sentences]\n",
    "        sentences_count = len(sentences)\n",
    "        \n",
    "        for sent in sentences_only_with_voc:\n",
    "            indexes = [self.w2v.word2id[word] for word in sent]\n",
    "            word_idf[indexes] += 1.0\n",
    "            \n",
    "        word_idf = np.log(sentences_count / (1 + word_idf).astype(float))     \n",
    "        return dict(zip(list(self.w2v.word2id.keys()), word_idf)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 pretrained word vectors\n",
      "Loaded  150736  sentences from sentences.txt\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "5 most similar BoV-mean sentences to : \" 1 smiling african american boy . \"\n",
      "!! Should increase pretrained word vectors to have a better result !!\n",
      "--------------------\n",
      "blond boy waterskiing .\n",
      "a small boy following 4 geese .\n",
      "boy flings mud at girl\n",
      "a boy jumps on another boy .\n",
      "a boy screams .\n",
      "---------------------------------------------\n",
      "score BoV-mean between : \" 1 man singing and 1 man playing a saxophone in a concert . \" and \" 10 people venture out to go crosscountry skiing . \"\n",
      "!! Should increase pretrained word vectors to have a better result !!\n",
      "--------------------\n",
      "0.5444733702595055\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "IDF dictionary built !\n",
      "5 most similar BoV-idf sentences to : \" 1 smiling african american boy . \"\n",
      "!! Should increase pretrained word vectors to have a better result !!\n",
      "--------------------\n",
      "a man rides a 4 wheeler in the desert .\n",
      "a man in black is juggling 3 flamed bottles .\n",
      "5 women and 1 man are smiling for the camera .\n",
      "3 males and 1 woman enjoying a sporting event\n",
      "a young boy in the water in 2 floating tubes .\n",
      "---------------------------------------------\n",
      "score BoV-idf between : \" 1 man singing and 1 man playing a saxophone in a concert . \" and \" 10 people venture out to go crosscountry skiing . \"\n",
      "--------------------\n",
      "0.5502735561241073\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=5000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "def from_txt_to_sent(path):\n",
    "    with open(path) as f:\n",
    "        sentences = f.readlines()\n",
    "    # remove whitespace characters like `\\n` at the end of each line\n",
    "    return [x.strip() for x in sentences]\n",
    "sentences = from_txt_to_sent(os.path.join(PATH_TO_DATA, 'sentences.txt'))\n",
    "print('Loaded ', len(sentences), ' sentences from sentences.txt')\n",
    "print('-'*45)\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = {} if True else s2v.build_idf(sentences)\n",
    "print('-'*45)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "i = 10\n",
    "print('5 most similar BoV-mean sentences to : \"', sentences[i], '\"')\n",
    "print('!! Should increase pretrained word vectors to have a better result !!')\n",
    "print('-'*20)\n",
    "s2v.most_similar('' if not sentences else sentences[i], sentences, verbose=False)# BoV-mean\n",
    "\n",
    "print('-'*45)\n",
    "i = 7\n",
    "j = 13\n",
    "print('score BoV-mean between : \"', '' if not sentences else sentences[i], '\" and \"', '' if not sentences else sentences[j], '\"')\n",
    "print('!! Should increase pretrained word vectors to have a better result !!')\n",
    "print('-'*20)\n",
    "score = s2v.score('' if not sentences else sentences[i], '' if not sentences else sentences[j])\n",
    "print(score)\n",
    "\n",
    "print('-'*45)\n",
    "print('-'*45)\n",
    "# Build the idf dictionary\n",
    "idf = s2v.build_idf(sentences)\n",
    "print('IDF dictionary built !')\n",
    "i = 10\n",
    "print('5 most similar BoV-idf sentences to : \"', sentences[i], '\"')\n",
    "print('!! Should increase pretrained word vectors to have a better result !!')\n",
    "print('-'*20)\n",
    "s2v.most_similar('' if not sentences else sentences[i], sentences, idf=idf, verbose=False)  # BoV-idf\n",
    "\n",
    "print('-'*45)\n",
    "i = 7\n",
    "j = 13\n",
    "print('score BoV-idf between : \"', '' if not sentences else sentences[i], '\" and \"', '' if not sentences else sentences[j], '\"')\n",
    "print('-'*20)\n",
    "score = s2v.score('' if not sentences else sentences[i], '' if not sentences else sentences[j], idf)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_wordvec(fname, nmax):\n",
    "        word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            deleted_words = 0 \n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word not in [',','.','#','?','!',',','\"',\"'\",':',';','(',')','/','</s>','-']: # Remove some punctuations\n",
    "                    word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                else:\n",
    "                    deleted_words += 1\n",
    "                if i == (nmax+deleted_words-1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(word2vec)))\n",
    "        return word2vec\n",
    "        \n",
    "en = load_wordvec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax = 50000)\n",
    "fr = load_wordvec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18964 common words.\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "\n",
    "common_words = [word for word in en.keys() if word in fr.keys()]\n",
    "X = np.array([fr[word] for word in common_words]).transpose()\n",
    "Y = np.array([en[word] for word in common_words]).transpose()\n",
    "\n",
    "print(len(common_words),'common words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from scipy import linalg\n",
    "\n",
    "U, S, V = linalg.svd(np.dot(Y, X.transpose()))\n",
    "W_fr2en = np.dot(U, V)\n",
    "W_en2fr = W_fr2en.transpose()\n",
    "\n",
    "# U.shape, S.shape, V.shape, X.shape, Y.shape, W_fr2en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From french to english : \n",
      "\n",
      "\n",
      "fleur  =>  flowers\n",
      "arbre  =>  trees\n",
      "café  =>  coffee\n",
      "effectivement  =>  obviously\n",
      "ordinateur  =>  computer\n",
      "vélo  =>  bicycle\n",
      "ville  =>  town\n",
      "\n",
      "\n",
      "From english to french :\n",
      "\n",
      "\n",
      "flower  => fleurs\n",
      "arm  => bras\n",
      "bottle  => bouteilles\n",
      "car  => voitures\n",
      "hat  => chapeau\n",
      "health  => santé\n",
      "state  => état\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def score(word_vec, word, lang=en):\n",
    "    return np.dot(word_vec, lang[word])/(np.linalg.norm(word_vec)*np.linalg.norm(lang[word]))\n",
    "\n",
    "def most_similar(word, lang=en):\n",
    "    most_similar_list = [score(word, w, lang) for w in lang.keys()]\n",
    "    most_similar_list_argsort = np.argsort(most_similar_list)\n",
    "    return list(lang.keys())[most_similar_list_argsort[-2]]\n",
    "\n",
    "def from_en_2_fr(word):\n",
    "    if word in en.keys():\n",
    "        word_vec_fr = np.dot(W_en2fr, en[word])\n",
    "        return most_similar(word_vec_fr, fr)\n",
    "    else:\n",
    "        return 'Word is not in English vocabulary !'\n",
    "\n",
    "def from_fr_2_en(word):\n",
    "    if word in fr.keys():\n",
    "        word_vec_en = np.dot(W_fr2en, fr[word])\n",
    "        return most_similar(word_vec_en, en)\n",
    "    else:\n",
    "        return 'Word is not in French vocabulary !'\n",
    "\n",
    "print('From french to english : ')\n",
    "print('\\n')\n",
    "list_fr_2_en = ['fleur', 'arbre', 'café', 'effectivement', 'ordinateur', 'vélo', 'ville']\n",
    "for w in list_fr_2_en:\n",
    "    print(w, ' => ', from_fr_2_en(w))\n",
    "    \n",
    "print('\\n')\n",
    "print('From english to french :')\n",
    "print('\\n')\n",
    "list_en_2_fr = ['flower', 'arm', 'bottle', 'car', 'hat', 'health', 'state']\n",
    "for w in list_en_2_fr:\n",
    "    print(w, ' =>', from_en_2_fr(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 pretrained sentences from train set\n",
      "Loaded 1101 pretrained sentences from dev set\n",
      "Loaded 2210 pretrained sentences from test set\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_train_dev(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "\n",
    "        sentences = [sent.split(' ', 1)[1] for sent in f]\n",
    "        score = [sent.split(' ', 1)[0] for sent in f]\n",
    "\n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences], score\n",
    "\n",
    "def load_test(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        \n",
    "        sentences = [sent for sent in f]\n",
    "        \n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences]\n",
    "    \n",
    "train_x, train_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), 'train')\n",
    "dev_x, dev_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), 'dev')\n",
    "test_x = load_test(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Average of word vectors done !\n",
      "IDF dictionary built !\n",
      "Weighted average of word vectors done !\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=50000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "train_xv = np.array(s2v.encode(train_x))\n",
    "dev_xv = np.array(s2v.encode(dev_x))\n",
    "test_xv = np.array(s2v.encode(test_x))\n",
    "print('Average of word vectors done !')\n",
    "\n",
    "#idf = s2v.build_idf(train_x+dev_x)\n",
    "print('IDF dictionary built !')\n",
    "\n",
    "#train_xi = np.array(s2v.encode(train_x, idf))\n",
    "#dev_xi = np.array(s2v.encode(dev_x, idf))\n",
    "#test_xi = np.array(s2v.encode(test_x, idf))\n",
    "print('Weighted average of word vectors done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = \"../../data/\"\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = 0  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  '' # find the right loss for multi-class classification\n",
    "optimizer        =  '' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "9\n",
      "28\n",
      "65\n",
      "126\n",
      "217\n",
      "344\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.33333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "364/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
