{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PATH_TO_DATA = \"../../data/\"\n",
    "PATH_TO_DATA = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.word2id = {}\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        #self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            j = 0\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word not in [',','.','#','?','!',',','\"',\"'\",':',';','(',')', '/']: # Remove punctuation\n",
    "                    self.word2vec[word] = np.fromstring(vec, sep=' ') \n",
    "                    self.word2id[word] = j\n",
    "                    j += 1\n",
    "                # Code to not differentiate word and word.lower()\n",
    "                #self.word2vec[word.lower()] = np.fromstring(vec, sep=' ') if not (word.lower() in self.word2vec.keys()) else (np.fromstring(vec, sep=' ')+self.word2vec[word.lower()])/2\n",
    "                #self.word2id[word.lower()] = i if not (word.lower() in self.word2id.keys()) else self.word2id[word.lower()]\n",
    "                if j == (nmax):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort\n",
    "        if w in self.word2vec.keys():\n",
    "            # Create a list with the score of each word\n",
    "            most_similar_list = [self.score(word,w) for word in self.word2vec.keys()]\n",
    "            # return the 5 most similar words\n",
    "            most_similar_list_argsort = np.argsort(most_similar_list)\n",
    "            return [self.id2word[most_similar_list_argsort[-k-2]] for k in range(K)]\n",
    "        else:\n",
    "            return w+' is missing of vocabulary'\n",
    "       \n",
    "    \n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        try:\n",
    "            return np.dot(self.word2vec[w1], self.word2vec[w2])/((np.linalg.norm(self.word2vec[w1]))*(np.linalg.norm(self.word2vec[w2])))\n",
    "        except KeyError as e:\n",
    "            return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0.7420295235998394\n",
      "['cats', 'kitty', 'kitten', 'feline', 'kitties']\n",
      "['dogs', 'puppy', 'Dog', 'doggie', 'canine']\n",
      "['dog', 'pooches', 'Dogs', 'doggies', 'canines']\n",
      "['france', 'Paris', 'london', 'berlin', 'tokyo']\n",
      "['austria', 'europe', 'german', 'berlin', 'poland']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "\n",
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # compute the mean of word2vec values for each words in a sent\n",
    "                #sentemb.append(np.mean([self.w2v.word2vec[word] for word in sent.split(' ') if word in self.w2v.word2vec.keys()], axis=0))\n",
    "                vectors = np.array([self.w2v.word2vec[word] for word in sent.split(' ') if word in self.w2v.word2vec.keys()])\n",
    "                if vectors.any():\n",
    "                    sentemb.append(np.mean(vectors, axis=0))\n",
    "                else:\n",
    "                    sentemb.append(np.zeros(300))\n",
    "                \n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                \n",
    "                # Create a vector with words of each sentences, and their weights from the idf dict\n",
    "                vectors = [self.w2v.word2vec[word] for word in sent.split(' ') if word in self.w2v.word2vec.keys()]\n",
    "                weights = [idf[word] for word in sent.split(' ') if word in self.w2v.word2vec.keys()]\n",
    "\n",
    "                if np.sum(weights)>0:\n",
    "                    sentemb.append(np.average(vectors, axis=0, weights=weights))\n",
    "                else:\n",
    "                    sentemb.append(np.zeros(300))\n",
    "                \n",
    "        return sentemb\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5, verbose=False):\n",
    "        # get most similar sentences and **print** them\n",
    "        # calculate the average vector for all words in every sentence/document \n",
    "        # and use cosine similarity between vectors\n",
    "        #keys = self.encode(sentences, idf)\n",
    "        #query = self.encode([s], idf)\n",
    "        \n",
    "        # Create many batches to limit memory usage\n",
    "        size_total = len(sentences)\n",
    "        batch_size = 3000\n",
    "        batch_step = int(size_total/batch_size)\n",
    "        batch_last = size_total - batch_size * batch_step\n",
    "        \n",
    "        most_similar_list = [] # Create a list of tuple (sentence, score)\n",
    "        \n",
    "        # Some verbose\n",
    "        time1=time.time()\n",
    "        timer1 = True\n",
    "        timer2 = True\n",
    "        timer3 = True\n",
    "        # Loop on batches       \n",
    "        for batch in range(batch_step):\n",
    "            # Add some timers\n",
    "            if verbose==True:\n",
    "                if (batch/batch_step > 1/4)&(timer1):\n",
    "                    time2=time.time()\n",
    "                    print('25% done in ', int(time2-time1), ' sec!')\n",
    "                    timer1=False\n",
    "                elif (batch/batch_step > 1/2)&(timer2):\n",
    "                    time3=time.time()\n",
    "                    print('50% done in ', int(time3-time2), ' sec!')\n",
    "                    timer2=False\n",
    "                elif (batch/batch_step > 3/4)&(timer3):\n",
    "                    time4=time.time()\n",
    "                    print('75% done in ', int(time4-time3), ' sec!')\n",
    "                    timer3=False\n",
    "            \n",
    "            # Create a list with scores of all words in the batch\n",
    "            most_similar_list_per_batch = []\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning) # Ignore a RuntimeWarning\n",
    "                for sent in sentences[batch*batch_size:(batch+1)*batch_size]:\n",
    "                    _score = self.score(sent, s, idf) # Compute the score between 2 sentences with self.score\n",
    "                    most_similar_list_per_batch.append(_score if np.any(_score>0) else 0) # Append the score to the list or append 0\n",
    "                \n",
    "                \n",
    "            most_similar_list_per_batch_argsort = np.argsort(most_similar_list_per_batch) # Create argsort list\n",
    "            \n",
    "            # Add the 6 similar sentences of this batch to a general list (6 to avoid an if condition to remove the actual sentence)\n",
    "            for k in range(K+1):\n",
    "                most_similar_list.append((sentences[batch*batch_size+most_similar_list_per_batch_argsort[-k-1]], most_similar_list_per_batch[most_similar_list_per_batch_argsort[-k-1]]))\n",
    "            \n",
    "        # Last batch (same as before)\n",
    "        if len(sentences[batch_size * batch_step:])>6: # If the last batch is longer than 6\n",
    "            most_similar_list_per_batch = []\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                for sent in sentences[batch_size * batch_step:]:\n",
    "                    _score = self.score(sent, s, idf)\n",
    "                    most_similar_list_per_batch.append(_score if np.any(_score>0) else 0)\n",
    "\n",
    "            most_similar_list_per_batch_argsort = np.argsort(most_similar_list_per_batch)\n",
    "        \n",
    "            for k in range(K+1):\n",
    "                most_similar_list.append((sentences[batch_size*batch_step+most_similar_list_per_batch_argsort[-k-1]], most_similar_list_per_batch[most_similar_list_per_batch_argsort[-k-1]]))\n",
    "        else: # Add all the last batch if his length is smaller than 6\n",
    "            for sent in sentences[batch_size * batch_step:]:\n",
    "                most_similar_list.append((sent, self.score(sent, s, idf) if np.any(self.score(sent, s, idf))>0 else 0))\n",
    "                \n",
    "                \n",
    "                \n",
    "        # Sort the most_similar_list (sentence, word) to keep the 5 highest scores\n",
    "        most_similar_list.sort(key=lambda tup: tup[1])\n",
    "        \n",
    "        # Some verbose\n",
    "        time4=time.time()\n",
    "        if verbose==True:\n",
    "            print(\"100% done in \", int(time4-time3), ' sec.')\n",
    "            print('Total time : ', int(time4-time1), ' sec.')\n",
    "            print('-'*30)\n",
    "            print('The 5 most similar sentences are :')\n",
    "        \n",
    "        # Get and Print the K similar sentences\n",
    "        K_similar_sentences = []\n",
    "        for k in range(K):\n",
    "            K_similar_sentences.append(most_similar_list[-k-2][0])\n",
    "            print(most_similar_list[-k-2][0])\n",
    "        \n",
    "        return K_similar_sentences\n",
    "    \n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        if s1 and s2:\n",
    "            return np.dot(self.encode([s1, s2], idf)[0], self.encode([s1, s2], idf)[1]) / (\n",
    "                (np.linalg.norm(self.encode([s1, s2], idf)[0] * np.linalg.norm(self.encode([s1, s2], idf)[1]))))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "        \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        # Let’s compute the Idf for every word in the vocabulary\n",
    "        word_idf = np.zeros(len(self.w2v.word2id))\n",
    "        sentences_only_with_voc = [[word for word in sent.split(' ') if word in self.w2v.word2id.keys()] for sent in sentences]\n",
    "        sentences_count = len(sentences)\n",
    "        \n",
    "        for sent in sentences_only_with_voc:\n",
    "            indexes = [self.w2v.word2id[word] for word in sent]\n",
    "            word_idf[indexes] += 1.0\n",
    "            \n",
    "        word_idf = np.log(sentences_count / (1 + word_idf).astype(float))     \n",
    "        return dict(zip(list(self.w2v.word2id.keys()), word_idf)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "Loaded  150736  sentences from sentences.txt\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "5 most similar BoV-mean sentences to : \" 1 smiling african american boy . \"\n",
      "--------------------\n",
      "an african american man smiling .\n",
      "a little african american boy and girl looking up .\n",
      "african american woman bouncing black basketball\n",
      "an afican american woman standing behind two small african american children .\n",
      "a girl in black hat holding an african american baby .\n",
      "---------------------------------------------\n",
      "score BoV-mean between : \" 1 man singing and 1 man playing a saxophone in a concert . \" and \" 10 people venture out to go crosscountry skiing . \"\n",
      "--------------------\n",
      "0.5204760325902832\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "IDF dictionary built !\n",
      "5 most similar BoV-idf sentences to : \" 1 smiling african american boy . \"\n",
      "--------------------\n",
      "an african american man smiling .\n",
      "a little african american boy and girl looking up .\n",
      "an african american man is sitting .\n",
      "an afican american woman standing behind two small african american children .\n",
      "a girl in black hat holding an african american baby .\n",
      "---------------------------------------------\n",
      "score BoV-idf between : \" 1 man singing and 1 man playing a saxophone in a concert . \" and \" 10 people venture out to go crosscountry skiing . \"\n",
      "--------------------\n",
      "0.43134943844335366\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "def from_txt_to_sent(path):\n",
    "    with open(path) as f:\n",
    "        sentences = f.readlines()\n",
    "    # remove whitespace characters like `\\n` at the end of each line\n",
    "    return [x.strip() for x in sentences]\n",
    "sentences = from_txt_to_sent(os.path.join(PATH_TO_DATA, 'sentences.txt'))\n",
    "print('Loaded ', len(sentences), ' sentences from sentences.txt')\n",
    "print('-'*45)\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = {} if True else s2v.build_idf(sentences)\n",
    "print('-'*45)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "i = 10\n",
    "print('5 most similar BoV-mean sentences to : \"', sentences[i], '\"')\n",
    "#print('!! Should increase pretrained word vectors to have a better result !!')\n",
    "print('-'*20)\n",
    "s2v.most_similar('' if not sentences else sentences[i], sentences, verbose=False)# BoV-mean\n",
    "\n",
    "print('-'*45)\n",
    "i = 7\n",
    "j = 13\n",
    "print('score BoV-mean between : \"', '' if not sentences else sentences[i], '\" and \"', '' if not sentences else sentences[j], '\"')\n",
    "#print('!! Should increase pretrained word vectors to have a better result !!')\n",
    "print('-'*20)\n",
    "score = s2v.score('' if not sentences else sentences[i], '' if not sentences else sentences[j])\n",
    "print(score)\n",
    "\n",
    "print('-'*45)\n",
    "print('-'*45)\n",
    "# Build the idf dictionary\n",
    "idf = s2v.build_idf(sentences)\n",
    "print('IDF dictionary built !')\n",
    "i = 10\n",
    "print('5 most similar BoV-idf sentences to : \"', sentences[i], '\"')\n",
    "#print('!! Should increase pretrained word vectors to have a better result !!')\n",
    "print('-'*20)\n",
    "s2v.most_similar('' if not sentences else sentences[i], sentences, idf=idf, verbose=False)  # BoV-idf\n",
    "\n",
    "print('-'*45)\n",
    "i = 7\n",
    "j = 13\n",
    "print('score BoV-idf between : \"', '' if not sentences else sentences[i], '\" and \"', '' if not sentences else sentences[j], '\"')\n",
    "print('-'*20)\n",
    "score = s2v.score('' if not sentences else sentences[i], '' if not sentences else sentences[j], idf)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_wordvec(fname, nmax):\n",
    "        word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            deleted_words = 0 # Add a counter to keep 50k words with deleted words\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word not in [',','.','#','?','!',',','\"',\"'\",':',';','(',')','/','</s>','-']: # Remove some punctuations\n",
    "                    word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                else:\n",
    "                    deleted_words += 1\n",
    "                if i == (nmax+deleted_words-1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(word2vec)))\n",
    "        return word2vec\n",
    "        \n",
    "en = load_wordvec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax = 50000)\n",
    "fr = load_wordvec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18964 common words.\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "\n",
    "common_words = [word for word in en.keys() if word in fr.keys()]\n",
    "X = np.array([fr[word] for word in common_words]).transpose()\n",
    "Y = np.array([en[word] for word in common_words]).transpose()\n",
    "\n",
    "print(len(common_words),'common words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 300), (300,), (300, 300), (300, 18964), (300, 18964), (300, 300))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from scipy import linalg\n",
    "\n",
    "U, S, V = linalg.svd(np.dot(Y, X.transpose()))\n",
    "W_fr2en = np.dot(U, V)\n",
    "W_en2fr = W_fr2en.transpose()\n",
    "\n",
    "U.shape, S.shape, V.shape, X.shape, Y.shape, W_fr2en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From french to english : \n",
      "\n",
      "\n",
      "fleur  =>  flowers, rosette, flowered, petals, fleur\n",
      "arbre  =>  trees, understory, seedlings, shrubs, bushes\n",
      "café  =>  coffee, cafe, cafés, chocolates, beans\n",
      "effectivement  =>  obviously, anyway, definitely, anyways, imho\n",
      "ordinateur  =>  computer, mainframe, workstation, programmable, workstations\n",
      "vélo  =>  bicycle, bicycles, bicycling, bikes, biking\n",
      "ville  =>  town, cities, suburb, towns, environs\n",
      "\n",
      "\n",
      "From english to french :\n",
      "\n",
      "\n",
      "flower  => fleurs, flowers, fleur, pétales, leaves\n",
      "arm  => bras, jambe, body, hanche, épaule\n",
      "bottle  => bouteilles, flacon, bottle, vodka, liqueur\n",
      "car  => voitures, automobile, porsche, automobiles, roadster\n",
      "hat  => chapeau, cowboy, shirt, trick, casquette\n",
      "health  => santé, nutrition, medical, hygiène, épidémiologie\n",
      "state  => état, etat, federal, government, états\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def score(word_vec, word, lang=en):\n",
    "    return np.dot(word_vec, lang[word])/(np.linalg.norm(word_vec)*np.linalg.norm(lang[word]))\n",
    "\n",
    "def most_similar(word, lang=en, K=3):\n",
    "    most_similar_list = [score(word, w, lang) for w in lang.keys()]\n",
    "    most_similar_list_argsort = np.argsort(most_similar_list)\n",
    "    #l = list(lang.keys())[most_similar_list_argsort[-K-1:-1]]\n",
    "    return [list(lang.keys())[most_similar_list_argsort[-k-2]] for k in range(K)]\n",
    "\n",
    "def from_en_2_fr(word, K=3):\n",
    "    if word in en.keys():\n",
    "        word_vec_fr = np.dot(W_en2fr, en[word])\n",
    "        l = most_similar(word_vec_fr, fr, K)\n",
    "        answer = ''\n",
    "        for w in l[:-1]:\n",
    "            answer += w+', '\n",
    "        answer += l[-1]\n",
    "        return answer\n",
    "    else:\n",
    "        return 'Word is not in English vocabulary !'\n",
    "\n",
    "def from_fr_2_en(word, K=3):\n",
    "    if word in fr.keys():\n",
    "        word_vec_en = np.dot(W_fr2en, fr[word])\n",
    "        l = most_similar(word_vec_en, en, K)\n",
    "        answer = ''\n",
    "        for w in l[:-1]:\n",
    "            answer += w+', '\n",
    "        answer += l[-1]\n",
    "        return answer\n",
    "    else:\n",
    "        return 'Word is not in French vocabulary !'\n",
    "\n",
    "\n",
    "print('From french to english : ')\n",
    "print('\\n')\n",
    "list_fr_2_en = ['fleur', 'arbre', 'café', 'effectivement', 'ordinateur', 'vélo', 'ville']\n",
    "for w in list_fr_2_en:\n",
    "    print(w, ' => ', from_fr_2_en(w, 5))\n",
    "    \n",
    "print('\\n')\n",
    "print('From english to french :')\n",
    "print('\\n')\n",
    "list_en_2_fr = ['flower', 'arm', 'bottle', 'car', 'hat', 'health', 'state']\n",
    "for w in list_en_2_fr:\n",
    "    print(w, ' =>', from_en_2_fr(w, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 pretrained sentences from train set\n",
      "Loaded 1101 pretrained sentences from dev set\n",
      "Loaded 2210 pretrained sentences from test set\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_train_dev(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "\n",
    "        score=[]\n",
    "        sentences=[]\n",
    "        for sent in f:\n",
    "            score.append(int(sent.split(' ', 1)[0]))\n",
    "            sentences.append(sent.split(' ', 1)[1])\n",
    "        #score = [sent.split(' ', 1)[0] for sent in f]\n",
    "        #sentences = [sent.split(' ', 1)[1] for sent in f]\n",
    "\n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences], np.array(score)\n",
    "\n",
    "def load_test(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        \n",
    "        sentences = [sent for sent in f]\n",
    "        \n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences]\n",
    "    \n",
    "train_x, train_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), 'train')\n",
    "dev_x, dev_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), 'dev')\n",
    "test_x = load_test(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 190000 pretrained word vectors\n",
      "Average of word vectors done !\n",
      "IDF dictionary built !\n",
      "Weighted average of word vectors done !\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=190000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "train_xv = np.array(s2v.encode(train_x))\n",
    "dev_xv = np.array(s2v.encode(dev_x))\n",
    "test_xv = np.array(s2v.encode(test_x))\n",
    "print('Average of word vectors done !')\n",
    "\n",
    "idf = s2v.build_idf(train_x+dev_x)\n",
    "print('IDF dictionary built !')\n",
    "\n",
    "train_xi = np.array(s2v.encode(train_x, idf))\n",
    "dev_xi = np.array(s2v.encode(dev_x, idf))\n",
    "test_xi = np.array(s2v.encode(test_x, idf))\n",
    "print('Weighted average of word vectors done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139   0   0   0   0]\n",
      " [  0 289   0   0   0]\n",
      " [  0   0 229   0   0]\n",
      " [  0   0   0 279   0]\n",
      " [  0   0   0   0 165]]\n",
      "\n",
      "\n",
      "Average of word vectors : \n",
      "LogisticRegression(C=0.16, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Train error :  0.4643024344569288\n",
      "Dev error :  0.4268846503178928\n",
      "Confusion matrix :\n",
      "[[ 11   6   4   2   0]\n",
      " [105 198  96  49  12]\n",
      " [  2   8  10   3   1]\n",
      " [ 18  77 113 210 111]\n",
      " [  3   0   6  15  41]]\n",
      "\n",
      "\n",
      "Weighted average of word vectors : \n",
      "LogisticRegression(C=0.14, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Train error :  0.4734316479400749\n",
      "Dev error :  0.4305177111716621\n",
      "Confusion matrix :\n",
      "[[ 19  10   3   2   0]\n",
      " [ 98 192  94  56  11]\n",
      " [  5  15  12   3   1]\n",
      " [ 13  67 114 197  99]\n",
      " [  4   5   6  21  54]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Function to create a logistic regression model. Return the fitted model\n",
    "def logReg(data, model_LR=LogisticRegression()):\n",
    "    train_x = data[0]\n",
    "    train_y = data[1]\n",
    "    dev_x = data[2]\n",
    "    dev_y = data[3]\n",
    "    model_LR.fit(train_x, train_y) # Fit the model\n",
    "    pred = model_LR.predict(dev_x) # Predict dev\n",
    "    print(model)\n",
    "    print('Train error : ', model_LR.score(train_x, train_y)) # Print score\n",
    "    print('Dev error : ', accuracy_score(pred, dev_y))\n",
    "    print('Confusion matrix :')\n",
    "    print(confusion_matrix(pred, dev_y))\n",
    "    print('\\n')\n",
    "    return model_LR\n",
    "\n",
    "''' After a grid_search on weighted/average, C & penalty, \n",
    " the best score on train/dev is reach with idf_average, l2 and C=0.14 '''\n",
    "\n",
    "# Selection of weighted dataset\n",
    "average_bov = [train_xv, train_y, dev_xv, dev_y]\n",
    "weighted_bov = [train_xi, train_y, dev_xi, dev_y]\n",
    "\n",
    "# Create and fit the model with optimized parameters, and print some metrics\n",
    "print(confusion_matrix(dev_y, dev_y))\n",
    "print('\\n')\n",
    "\n",
    "print('Average of word vectors : ')\n",
    "model = LogisticRegression(penalty='l2', C=0.16)\n",
    "model = logReg(average_bov, model)\n",
    "\n",
    "print('Weighted average of word vectors : ')\n",
    "model = LogisticRegression(penalty='l2', C=0.14)\n",
    "model = logReg(weighted_bov, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved !\n"
     ]
    }
   ],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# Prediction\n",
    "test_y = model.predict(test_xi)\n",
    "\n",
    "# Saving results\n",
    "file = open('data/logreg_bov_y_test_sst.txt', 'w')\n",
    "for l in test_y:\n",
    "    file.write(str(l))\n",
    "    file.write(\"\\n\")\n",
    "file.close()\n",
    "print('Results saved !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Score on train :  0.4868913857677903\n",
      "Score on dev :  0.40599455040871935\n",
      "[[ 25  19  16   5   2]\n",
      " [ 88 170  75  42   9]\n",
      " [  8  27  21  20   2]\n",
      " [ 11  62  99 166  87]\n",
      " [  7  11  18  46  65]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# Different test with MultinomialNB, LinearSVC and RandomForest shows that LinearSVC has better results\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "def fitting_other_models(data, model):\n",
    "    train_x = data[0]\n",
    "    train_y = data[1]\n",
    "    dev_x = data[2]\n",
    "    dev_y = data[3]\n",
    "    model.fit(train_x, train_y) # Fit the model\n",
    "    pred = model.predict(dev_x) # Predict dev\n",
    "    print(model)\n",
    "    print('Score on train : ', model.score(train_x, train_y)) # Print score\n",
    "    print('Score on dev : ', accuracy_score(pred, dev_y))\n",
    "    print(confusion_matrix(pred, dev_y))\n",
    "    print('\\n')\n",
    "    return model\n",
    "\n",
    "weighted_bov = [train_xi, train_y, dev_xi, dev_y]\n",
    "\n",
    "# Create and fit the model with optimized parameters, and print some metrics\n",
    "model = LinearSVC()\n",
    "model = fitting_other_models(weighted_bov, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Romain\\Anaconda3V2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 pretrained sentences from train set\n",
      "Loaded 1101 pretrained sentences from dev set\n",
      "Loaded 2210 pretrained sentences from test set\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "#PATH_TO_DATA = \"../../data/\"\n",
    "PATH_TO_DATA = 'data/'\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_train_dev(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "\n",
    "        score=[]\n",
    "        sentences=[]\n",
    "        for sent in f:\n",
    "            score.append(int(sent.split(' ', 1)[0]))\n",
    "            sentences.append(sent.split(' ', 1)[1])\n",
    "\n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences], np.array(score)\n",
    "\n",
    "def load_test(fname, name=''):\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        \n",
    "        sentences = [sent for sent in f]\n",
    "        \n",
    "    print('Loaded %s pretrained sentences from %s set' % (len(sentences), name))\n",
    "    return [x.strip() for x in sentences]\n",
    "\n",
    "def get_dummies(data): # transform np.array into dummy variable to fit models\n",
    "    result = np.array([[0 for i in range(data.min(), data.max()+1)] for i in data])\n",
    "    for i, k in enumerate(data):\n",
    "        result[i][k] = 1\n",
    "    return result\n",
    "    \n",
    "train_x, train_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'), 'train')\n",
    "dev_x, dev_y = load_train_dev(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'), 'dev')\n",
    "test_x = load_test(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'), 'test')\n",
    "\n",
    "train_y = get_dummies(train_y)\n",
    "dev_y = get_dummies(dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing import text\n",
    "\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "train_voc_size = round(len([word for sent in train_x for word in sent.split(' ')])*1.5)\n",
    "\n",
    "train_int = [text.one_hot(text=sent, n=train_voc_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \") \n",
    "             for sent in train_x]\n",
    "dev_int = [text.one_hot(text=sent, n=train_voc_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \") \n",
    "           for sent in dev_x]\n",
    "test_int = [text.one_hot(text=sent, n=train_voc_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \") \n",
    "            for sent in test_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_len = len(max(train_int, key=len))\n",
    "\n",
    "train_int = sequence.pad_sequences(train_int, maxlen=max_len)\n",
    "dev_int = sequence.pad_sequences(dev_int, maxlen=max_len)\n",
    "test_int = sequence.pad_sequences(test_int, maxlen=max_len)\n",
    "\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 300  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = train_voc_size  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         73603200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 73,696,965\n",
      "Trainable params: 73,696,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'rmsprop' #'rmsprop' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/8\n",
      "8544/8544 [==============================] - 203s 24ms/step - loss: 1.5550 - acc: 0.2983 - val_loss: 1.4749 - val_acc: 0.3606\n",
      "Epoch 2/8\n",
      "8544/8544 [==============================] - 186s 22ms/step - loss: 1.3502 - acc: 0.4068 - val_loss: 1.3653 - val_acc: 0.3833\n",
      "Epoch 3/8\n",
      "8544/8544 [==============================] - 187s 22ms/step - loss: 1.1381 - acc: 0.4692 - val_loss: 1.3832 - val_acc: 0.3797\n",
      "Epoch 4/8\n",
      "8544/8544 [==============================] - 200s 23ms/step - loss: 0.9818 - acc: 0.5405 - val_loss: 1.4529 - val_acc: 0.3915\n",
      "Epoch 5/8\n",
      "8544/8544 [==============================] - 202s 24ms/step - loss: 0.8328 - acc: 0.6520 - val_loss: 1.5619 - val_acc: 0.3924\n",
      "Epoch 6/8\n",
      "8544/8544 [==============================] - 214s 25ms/step - loss: 0.7013 - acc: 0.7440 - val_loss: 1.7623 - val_acc: 0.4042\n",
      "Epoch 7/8\n",
      "8544/8544 [==============================] - 191s 22ms/step - loss: 0.5708 - acc: 0.8023 - val_loss: 1.8413 - val_acc: 0.3787\n",
      "Epoch 8/8\n",
      "8544/8544 [==============================] - 194s 23ms/step - loss: 0.4557 - acc: 0.8447 - val_loss: 2.0482 - val_acc: 0.3778\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22a66130780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "x_train = np.array(train_int)\n",
    "y_train = train_y\n",
    "x_val = np.array(dev_int)\n",
    "y_val = dev_y\n",
    "\n",
    "bs = 64 # 267\n",
    "n_epochs = 8\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(history.history['acc'], c='blue', alpha=0.8, label='Train accuracy')\n",
    "plt.plot(history.history['val_acc'], c='green', alpha=0.8, label='Dev accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([i for i in range(n_epochs)], [i+1 for i in range(n_epochs)])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved !\n"
     ]
    }
   ],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# Print confusion matrix on dev dataset, and score.\n",
    "verbose = False\n",
    "if verbose == True:\n",
    "    dev_y_ = [row.argmax() for row in dev_y]\n",
    "    pred_dev = model.predict_classes(dev_int)\n",
    "    print(confusion_matrix(dev_y_, pred_test))\n",
    "    print(accuracy_score(dev_y_, pred_test))\n",
    "\n",
    "# Predict classes of test_int\n",
    "pred_test = model.predict_classes(test_int)\n",
    "\n",
    "# Saving results\n",
    "file = open('data/logreg_lstm_y_test_sst.txt', 'w')\n",
    "for l in pred_test:\n",
    "    file.write(str(l))\n",
    "    file.write(\"\\n\")\n",
    "file.close()\n",
    "print(\"Results saved !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
